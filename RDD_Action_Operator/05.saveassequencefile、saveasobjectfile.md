## saveAsSequenceFile、saveAsObjectFile
### saveAsTextFile
def saveAsTextFile(path: String): Unit

def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit

saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。

codec参数可以指定压缩的类名。

```
var rdd1 = sc.makeRDD(1 to 10,2)
scala> rdd1.saveAsTextFile("hdfs:///user/apple/testspark") //保存到HDFS
scala> rdd1.saveAsTextFile("file:///Users/apple/testspark") //保存到本地

//由于设置为两个分区，因此保存的路径中，有part-00000,part-00000
```

指定压缩格式保存

```
//需要将使用的压缩工具包导入
rdd1.saveAsTextFile("hdfs:///user/apple/testspark1",classOf[com.hadoop.compression.lzo.LzopCodec])
```

### saveAsSequenceFile

saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。

用法同saveAsTextFile。

spark 1.6.1 没有改方法

### saveAsObjectFile

def saveAsObjectFile(path: String): Unit

saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。

对于HDFS，默认采用SequenceFile保存。

```
rdd1.saveAsObjectFile("/user/apple/sparktest02")

hadoop fs -cat /user/apple/sparktest02/part-00000
SEQ !org.apache.hadoop.io.NullWritable"org.apache.hadoop.io.BytesWritableT
```